# Literaturhinweise

- Bhattacharyya, Mehul et al. (2023). High rates of fabricated and inaccurate references in chatgpt-generated medical content. Curēus (Palo Alto, CA), 15(5). S. e39238. https://doi.org/10.7759/cureus.39238
- Dahl, Matthew et al. (2024). Large legal fictions: profiling legal hallucinations in large language models. The journal of legal analysis, 16(1). S. 64-93. https://doi.org/10.1093/jla/laae003
- Huang, Lei et al. (2025). A survey on hallucination in large language models: principles, taxonomy, challenges, and open questions. ACM transactions on information systems, 43(2). S. 1-55. https://doi.org/10.1145/3703155
- Tao, Yan et al. (2024). Cultural bias and cultural alignment of large language models. PNAS nexus, 3(9). S. pgae346-9. https://doi.org/10.1093/pnasnexus/pgae346
- Zhou, Jiacheng et al. (2025). Integrating AI into clinical education: evaluating general practice trainees’ proficiency in distinguishing AI-generated hallucinations and impacting factors. BMC medical education, 25(1). S. 406-9. https://doi.org/10.1186/s12909-025-06916-2

# LinkedIn

Post 4

[47% frei erfunden. 46% falsch zitiert. Nur 7% korrekt.](https://www.linkedin.com/posts/peter-kocmann_ki-lernpsychologie-ailiteracy-activity-7410819209074569216-Zg2z?utm_source=share&utm_medium=member_desktop&rcm=ACoAAFX1Z50BZ2ggsdvzF0jid6ENIzThEer2oz0)
